nb_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
# Create a tuning grid for hyperparameters
tuning_grid <- grid_regular(
Laplace(range = c(0.1, 10)),
smoothness(range = c(0.1, 5)),
levels = 10
)
# Cross-validation setup
folds <- vfold_cv(train, v = 5)
# Tune the model using cross-validation
cv_results <- tune_grid(
nb_workflow,
resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc, accuracy)
)
# Select the best hyperparameters based on AUC
best_tune <- select_best(cv_results, metric = "roc_auc")
# Finalize the workflow
final_workflow <- finalize_workflow(nb_workflow, best_tune)
# Fit the final model to the training data
final_fit <- fit(final_workflow, data = train)
# Predict on the test data
nb_preds <- predict(final_fit, new_data = test, type = "class")
# Prepare the submission file
nb_submission <- test %>%
select(Id) %>%
bind_cols(Predicted = nb_preds$.pred_class)
# Write submission file
vroom_write(nb_submission, file = "NB_Submission.csv", delim = ",")
View(tuning_grid)
View(train)
nb_preds <- predict(final_fit, new_data = test, type = "class")
# Prepare the submission file with the correct column names
nb_submission <- test %>%
select(Id) %>% # Ensure 'Id' is selected from the test dataset
bind_cols(Cover_Type = nb_preds$.pred_class) # Rename the predicted column to 'Cover_Type'
# Write submission file with the expected format
vroom_write(nb_submission, file = "NB_Submission.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
my_recipe1 <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # Perform PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Apply SMOTE while all columns are numeric
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area columns to factors afterward
step_mutate_at(starts_with("Soil_Type"), fn = as.factor)
bake_prep <- prep(my_recipe1)
baked <- bake(bake_prep, new_data = NULL)
rf_spec <- rand_forest(
mtry = tune(),
trees = 500,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe1)
cv_splits <- vfold_cv(train, v = 3)
rf_grid <- grid_regular(
mtry(range = c(2, 15)),
min_n(range = c(5, 20)),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./RandomForestNEW.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area to factors
step_mutate_at(starts_with("Soil_Type"), fn = as.factor)
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size(range = c(0.5, 1)),
levels = 5
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
# Select Best Model
best_boosted <- boosted_results %>%
select_best(metric = "accuracy")
# Final Workflow
final_boosted_workflow <- boosted_workflow %>%
finalize_workflow(best_boosted)
# Fit Final Model
final_boosted_model <- final_boosted_workflow %>%
fit(data = train)
# Predictions for Submission
test_predictions <- predict(final_boosted_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./BoostedTreesSubmission.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area to factors
step_mutate_at(starts_with("Soil_Type"), fn = as.factor)
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size(range = c(0.5, 1)),
levels = 5
)
# Grid for Tuning
boosted_grid <- grid_latin_hypercube(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Correct specification
size = 20  # Specify the number of combinations
)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Correct specification
size = 20  # Specify the number of combinations
)
# Grid for Tuning
boosted_grid <- grid_space_filling(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Correct specification
size = 20  # Specify the number of combinations
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
# Select Best Model
best_boosted <- boosted_results %>%
select_best(metric = "accuracy")
# Predictions for Submission
test_predictions <- predict(final_boosted_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./BoostedTreesSubmission.csv", delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area to factors
step_mutate_at(starts_with("Soil_Type"), fn = as.factor)
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_dummy(starts_with("Wilderness_Area"), one_hot = TRUE)
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert to factors
step_dummy(starts_with("Wilderness_Area"), one_hot = TRUE)  # One-hot encode Wilderness_Area
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 1000)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
# Grid for Tuning
boosted_grid <- grid_regular(
trees = 200,
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(200),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 200)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
# Recipe
boosted_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Balance classes using SMOTE
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert to factors
step_dummy(starts_with("Wilderness_Area"), one_hot = TRUE)  # One-hot encode Wilderness_Area
boosted_recipe1 <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%  # Normalize numeric predictors
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area to factors
step_dummy(starts_with("Wilderness_Area"), one_hot = TRUE)  # One-hot encode Wilderness_Area
# Boosted Tree Specification
boosted_spec <- boost_tree(
trees = tune(),              # Number of trees
tree_depth = tune(),         # Depth of trees
learn_rate = tune(),         # Learning rate
mtry = tune(),               # Number of predictors to randomly sample
loss_reduction = tune(),     # Minimum reduction in loss for a split
sample_size = tune()         # Proportion of training data used in each tree
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# Workflow
boosted_workflow <- workflow() %>%
add_model(boosted_spec) %>%
add_recipe(boosted_recipe)
# Cross-Validation
cv_splits <- vfold_cv(train, v = 5)
# Grid for Tuning
boosted_grid <- grid_regular(
trees(range = c(100, 200)),
tree_depth(range = c(3, 10)),
learn_rate(range = c(0.01, 0.3)),
mtry(range = c(2, 10)),
loss_reduction(range = c(0, 10)),
sample_size = sample_prop(range = c(0.5, 1)),  # Proper proportion specification
levels = 5  # Number of levels for each hyperparameter
)
# Hyperparameter Tuning
boosted_results <- tune_grid(
boosted_workflow,
resamples = cv_splits,
grid = boosted_grid,
metrics = metric_set(accuracy, roc_auc)
)
