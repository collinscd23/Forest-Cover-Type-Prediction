submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestBATCH.csv",
delim = ",")
# Load necessary libraries
library(dplyr)
# Load necessary libraries
library(dplyr)
# Load the dataset (replace the path with your actual file location)
nba_data <- read.csv("/Users/carsoncollins/Desktop/Stats 234/archive/NBA_2024_per_game(26-01-2024).csv")
# Step 1: Identify unique teams
unique_teams <- unique(nba_data$Tm)
# Step 2: Perform cluster sampling (select 5 random teams as clusters)
set.seed(42)  # For reproducibility
sampled_teams <- sample(unique_teams, size = 5, replace = FALSE)
# Step 3: Filter the dataset for the sampled teams
cluster_sample <- nba_data %>% filter(Tm %in% sampled_teams)
# Step 4: Compute the point estimate (average points per game for the sampled teams)
point_estimate <- mean(cluster_sample$PTS, na.rm = TRUE)
# Step 5: Calculate the 95% confidence interval
n <- nrow(cluster_sample)
std_error <- sd(cluster_sample$PTS, na.rm = TRUE) / sqrt(n)
# Use the t-distribution to calculate the confidence interval
alpha <- 0.05
t_value <- qt(1 - alpha / 2, df = n - 1)
ci_lower <- point_estimate - t_value * std_error
ci_upper <- point_estimate + t_value * std_error
# Output results
cat("Point Estimate (Average PTS):", point_estimate, "\n")
cat("95% Confidence Interval:", ci_lower, "-", ci_upper, "\n")
ssh ccarsonj@becker.byu.edu
ssh ccarsonj@becker.byu.edu
qnorm(.025,.975)
qnorm(.025)
qnorm(.975)
qnorm(.05)
qnorm(.95)
qchisq(.025)
qchisq(.025)
qchisq(1,.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.05)
qchisq(.05,1)
qchisq(.95,1)
install.packages("themis")
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
library(themis)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data=train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_dummy(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = 0.9) %>%
step_smote(all_outcomes(), neighbors=4)
prepped_recipe <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),
trees(range = c(300,1000)), # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
View(predictions)
View(param_grid)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test, type = "prob")
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestTUNE.csv",
delim = ",")
install.packages("maxLik")
batavg <- read.csv("FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
library(maxLik)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
library(tidyverse)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
ba <- batavg$BattingAverage
loglik <- function(theta, x){
if (any(theta = 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- macLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
alpha.hat <- coef(ml)[1]
beta.hat <- coef(ml)[2]
library(maxLik)
library(tidyverse)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
ba <- batavg$BattingAverage
loglik <- function(theta, x){
if (any(theta = 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
loglik <- function(theta, x){
if (any(theta <= 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
loglik <- function(theta, x){
if (any(theta <= 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
alpha.hat <- coef(ml)[1]
beta.hat <- coef(ml)[2]
alpha.hat
beta.hat
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
library(tidyverse)
library(tidymodels)
library(vroom)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
View(train)
library(tidyverse)
library(tidymodels)
library(vroom)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_target_encode(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_target_encode(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
library(recipes)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_target_encode(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
bake_prep <- prep(my_recipe)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
bake_prep <- prep(my_recipe)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
bake_prep <- prep(my_recipe)
baked <- bake(bake_prep, new_data = NULL)
View(baked)
rf_model <- rand_forest(
mode = "regression",
trees = tune(),
min_n = tune()
) %>%
set_engine("ranger")
rf_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_model)
smape <- metric_set(smape)
cv_folds <- vfold_cv(storeItem, v = 5)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
bake_prep <- prep(my_recipe)
baked <- bake(bake_prep, new_data = NULL)
rf_spec <- rand_forest(
mtry = tune(),
trees = 500,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
set.seed(123)
cv_splits <- vfold_cv(train, v = 5)
rf_grid <- grid_regular(
mtry(range = c(2, 10)),
min_n(range = c(5, 15)),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best("accuracy")
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
write_csv(submission, "submission.csv")
head(submission)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
bake_prep <- prep(my_recipe)
baked <- bake(bake_prep, new_data = NULL)
rf_spec <- rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe)
cv_splits <- vfold_cv(train, v = 5)
rf_grid <- grid_regular(
mtry(range = c(2, 10)),
min_n(range = c(5, 15)),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
write_csv(submission, "RandomForest.csv")
head(submission)
