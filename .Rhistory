trees = 200) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),  # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test)
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestBATCH.csv",
delim = ",")
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test, type = "prob")
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestBATCH.csv",
delim = ",")
# Load necessary libraries
library(dplyr)
# Load necessary libraries
library(dplyr)
# Load the dataset (replace the path with your actual file location)
nba_data <- read.csv("/Users/carsoncollins/Desktop/Stats 234/archive/NBA_2024_per_game(26-01-2024).csv")
# Step 1: Identify unique teams
unique_teams <- unique(nba_data$Tm)
# Step 2: Perform cluster sampling (select 5 random teams as clusters)
set.seed(42)  # For reproducibility
sampled_teams <- sample(unique_teams, size = 5, replace = FALSE)
# Step 3: Filter the dataset for the sampled teams
cluster_sample <- nba_data %>% filter(Tm %in% sampled_teams)
# Step 4: Compute the point estimate (average points per game for the sampled teams)
point_estimate <- mean(cluster_sample$PTS, na.rm = TRUE)
# Step 5: Calculate the 95% confidence interval
n <- nrow(cluster_sample)
std_error <- sd(cluster_sample$PTS, na.rm = TRUE) / sqrt(n)
# Use the t-distribution to calculate the confidence interval
alpha <- 0.05
t_value <- qt(1 - alpha / 2, df = n - 1)
ci_lower <- point_estimate - t_value * std_error
ci_upper <- point_estimate + t_value * std_error
# Output results
cat("Point Estimate (Average PTS):", point_estimate, "\n")
cat("95% Confidence Interval:", ci_lower, "-", ci_upper, "\n")
ssh ccarsonj@becker.byu.edu
ssh ccarsonj@becker.byu.edu
qnorm(.025,.975)
qnorm(.025)
qnorm(.975)
qnorm(.05)
qnorm(.95)
qchisq(.025)
qchisq(.025)
qchisq(1,.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.025)
qchisq(.025,1)
qchisq(.975,1)
qchisq(.05)
qchisq(.05,1)
qchisq(.95,1)
install.packages("themis")
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
library(tidymodels)
library(embed)
library(vroom)
library(reshape2)
library(lme4)
library(kknn)
library(themis)
# Load train and test datasets
train <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/train.csv")
test <- vroom("/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/test.csv")
# Convert ACTION to factor in train dataset
train$ACTION <- as.factor(train$ACTION)
# Set up the recipe
my_recipe <- recipe(ACTION ~ ., data=train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_dummy(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold = 0.9) %>%
step_smote(all_outcomes(), neighbors=4)
prepped_recipe <- prep(my_recipe)
baked <- bake(prep, new_data = train)
# Specify the random forest model with tunable parameters
my_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow
randForest_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
# Set up cross-validation (5-fold CV)
cv_splits <- vfold_cv(train, v = 5)
# Create a grid of tuning parameters
param_grid <- grid_regular(
mtry(range = c(1, 5)),  # Reduced range for mtry
min_n(range = c(2, 10)),
trees(range = c(300,1000)), # Set specific range for min_n
levels = 3               # Reduce number of levels
)
# Tune the random forest model with cross-validation
tuned_results <- tune_grid(
randForest_wf,
resamples = cv_splits,
grid = param_grid,
metrics = metric_set(roc_auc),
control = control_grid(verbose = TRUE)
)
View(predictions)
View(param_grid)
# Extract the best parameters based on ROC AUC
best_params <- select_best(tuned_results, metric = "roc_auc")
# Finalize the workflow with the best parameters
final_wf <- finalize_workflow(
randForest_wf,
best_params
)
# Fit the final workflow on the entire training data
final_fit <- fit(final_wf, data = train)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test, type = "prob")
submission <- test %>%
dplyr::select(id) %>%
mutate(ACTION = test_predictions$.pred_1)
# Save the submission file
vroom_write(submission,
path = "/Users/carsoncollins/Desktop/Stats348/AmazonEmployeeAcess/Rand_ForestTUNE.csv",
delim = ",")
install.packages("maxLik")
batavg <- read.csv("FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
library(maxLik)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
library(tidyverse)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
ba <- batavg$BattingAverage
loglik <- function(theta, x){
if (any(theta = 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- macLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
alpha.hat <- coef(ml)[1]
beta.hat <- coef(ml)[2]
library(maxLik)
library(tidyverse)
batavg <- read_csv(file = "FirstName,LastName,Team,League,AtBats,BattingAverage,SalaryMillions
Spencer,Torkelson,DET,AL,360,0.203,0.7
Darin,Ruf,SFN,NL,268,0.216,3
Adam,Frazier,SEA,AL,541,0.238,7.5
Cavan,Biggio,TOR,AL,257,0.202,2.1225
Bryce,Harper,PHI,NL,370,0.286,26
Eric,Haase,DET,AL,323,0.254,0.7104
Jose,Abreu,CHA,AL,601,0.304,18
Austin,Hedges,CLE,AL,294,0.163,4
Corey,Seager,TEX,AL,593,0.245,32.5
Teoscar,Hernandez,TOR,AL,499,0.267,10.65
Byron,Buxton,MIN,AL,340,0.224,9
Salvador,Perez,KCA,AL,445,0.254,18
Andrew,Velazquez,LAA,AL,322,0.196,0.7
Eric,Hosmer,SDN,NL,335,0.272,20
Evan,Longoria,SFN,NL,266,0.244,19.5
Juan,Yepez,SLN,NL,253,0.253,0.7")
ba <- batavg$BattingAverage
loglik <- function(theta, x){
if (any(theta = 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
loglik <- function(theta, x){
if (any(theta <= 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
loglik <- function(theta, x){
if (any(theta <= 0))
NA
else
dbeta(x, theta[1], theta[2], log = TRUE)
}
ml <- maxLik(loglik,
start = c(shape1 = .5, shape2 =.5),
x=ba)
ml
alpha.hat <- coef(ml)[1]
beta.hat <- coef(ml)[2]
alpha.hat
beta.hat
qnorm(.95,7,sqrt(.25))
pnorm(7.822427, 7.5, sqrt(.25))
pnorm(7.822427, 8.0, sqrt(.25))
pnorm(7.822427, 8.5, sqrt(.25))
pnorm(7.822427, 9.0, sqrt(.25))
# Define the parameters
x <- 7.822427
means <- c(7.5, 8.0, 8.5, 9.0)
std_dev <- sqrt(0.25)
# Calculate probabilities
probabilities <- pnorm(x, mean = means, sd = std_dev)
# Plotting
plot(means, probabilities, type = "o", col = "blue", pch = 16,
xlab = "Mean", ylab = "CDF Value",
main = "CDF Values for Different Means")
# Number of households surveyed and willing to pay $20 or more
n_households <- 438
n_willing <- 23
# Proportion estimate
p_hat <- n_willing / n_households
p_hat
# Standard error of the proportion
se <- sqrt((p_hat * (1 - p_hat)) / n_households)
# 95% confidence interval using the normal approximation
z <- 1.96  # z-value for 95% confidence level
lower_bound <- p_hat - z * se
upper_bound <- p_hat + z * se
# Margin of error
margin_of_error <- z * se
# Print results
cat("Proportion (p_hat):", p_hat, "\n")
cat("Margin of Error:", margin_of_error, "\n")
cat("95% Confidence Interval:", "(", lower_bound, ",", upper_bound, ")\n")
# Number of households surveyed and willing to pay $20 or more
n_households <- 438
n_willing <- 23
# Proportion estimate
p_hat <- n_willing / n_households
p_hat
# Standard error of the proportion
se <- sqrt((p_hat * (1 - p_hat)) / n_households)
# 95% confidence interval using the normal approximation
z <- 1.96  # z-value for 95% confidence level
lower_bound <- p_hat - z * se
upper_bound <- p_hat + z * se
# Margin of error
margin_of_error <- z * se
# Print results
cat("Proportion (p_hat):", p_hat, "\n")
cat("Margin of Error:", margin_of_error, "\n")
cat("95% Confidence Interval:", "(", lower_bound, ",", upper_bound, ")\n")
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
View(test)
train$Cover_Type <- as.factor(train$Cover_Type)
View(test)
View(train)
library(tidyverse)
library(tidymodels)
library(vroom)
library(themis)
library(recipes)
setwd("~/Desktop/Stats348/Forest-Cover-Type-Prediction")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$Cover_Type <- as.factor(train$Cover_Type)
my_recipe <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_mutate(
Hydro_Euclidean_Dist = sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2),
Hillshade_Interaction = Hillshade_9am * Hillshade_Noon * Hillshade_3pm,
Aspect_sin = sin(Aspect * pi / 180),
Aspect_cos = cos(Aspect * pi / 180),
Terrain_Roughness = Slope * Elevation,
Relative_Distance_Water_Road = Horizontal_Distance_To_Hydrology - Horizontal_Distance_To_Roadways
) %>%
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%
step_mutate_at(starts_with("Soil_Type"), fn = as.factor) %>%
step_dummy(starts_with("Wilderness_Area"), starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%
step_smote(Cover_Type)
my_recipe1 <- recipe(Cover_Type ~ ., data = train) %>%
step_zv(all_predictors()) %>%  # Remove zero-variance columns
step_normalize(all_numeric_predictors(), -all_outcomes(), -starts_with("Wilderness_Area"), -starts_with("Soil_Type")) %>%
step_pca(starts_with("Soil_Type"), num_comp = 5) %>%  # Perform PCA on Soil_Type columns
step_smote(Cover_Type) %>%  # Apply SMOTE while all columns are numeric
step_mutate_at(starts_with("Wilderness_Area"), fn = as.factor) %>%  # Convert Wilderness_Area columns to factors afterward
step_mutate_at(starts_with("Soil_Type"), fn = as.factor)
my_recipe2 <- recipe(Cover_Type~., data = train) %>%
step_impute_median(contains("Soil_Type")) %>%
step_mutate(SoilType = pmax(
Soil_Type1, Soil_Type2 * 2, Soil_Type3 * 3, Soil_Type4 * 4, Soil_Type5 * 5,
Soil_Type6 * 6, Soil_Type7 * 7, Soil_Type8 * 8, Soil_Type9 * 9, Soil_Type10 * 10,
Soil_Type11 * 11, Soil_Type12 * 12, Soil_Type13 * 13, Soil_Type14 * 14, Soil_Type15 * 15,
Soil_Type16 * 16, Soil_Type17 * 17, Soil_Type18 * 18, Soil_Type19 * 19, Soil_Type20 * 20,
Soil_Type21 * 21, Soil_Type22 * 22, Soil_Type23 * 23, Soil_Type24 * 24, Soil_Type25 * 25,
Soil_Type26 * 26, Soil_Type27 * 27, Soil_Type28 * 28, Soil_Type29 * 29, Soil_Type30 * 30,
Soil_Type31 * 31, Soil_Type32 * 32, Soil_Type33 * 33, Soil_Type34 * 34, Soil_Type35 * 35,
Soil_Type36 * 36, Soil_Type37 * 37, Soil_Type38 * 38, Soil_Type39 * 39, Soil_Type40 * 40,
na.rm = TRUE)) %>%
step_mutate(WildernessArea = pmax(Wilderness_Area1, Wilderness_Area2 * 2, Wilderness_Area3 * 3, Wilderness_Area4 * 4, na.rm = TRUE)) %>%
step_rm(contains("Soil_Type")) %>%
step_rm(contains("Wilderness_Area")) %>%
step_rm(Id) %>%
step_mutate(Total_Distance_To_Hydrology = sqrt(Horizontal_Distance_To_Hydrology**2 + Vertical_Distance_To_Hydrology**2)) %>%
step_mutate(Elevation_Vertical_Hydrology = Vertical_Distance_To_Hydrology * Elevation) %>%
step_mutate(Hydrology_Fire = Horizontal_Distance_To_Hydrology * Horizontal_Distance_To_Fire_Points) %>%
step_mutate(Hydrology_Roadways = Horizontal_Distance_To_Hydrology * Horizontal_Distance_To_Roadways) %>%
step_mutate(Roadways_Fire = Horizontal_Distance_To_Roadways * Horizontal_Distance_To_Fire_Points) %>%
step_mutate_at(c(SoilType, WildernessArea), fn = factor) %>%
step_zv(all_predictors())
bake_prep <- prep(my_recipe2)
baked <- bake(bake_prep, new_data = NULL)
rf_spec <- rand_forest(
mtry = tune(),
trees = 250,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe2)
cv_splits <- vfold_cv(train, v = 2)
rf_grid <- grid_regular(
mtry(range = c(2, 15)),
min_n(),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./RandomForestWedn.csv", delim = ",")
bake_prep <- prep(my_recipe2)
baked <- bake(bake_prep, new_data = NULL)
rf_spec <- rand_forest(
mtry = tune(),
trees = 500,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe2)
cv_splits <- vfold_cv(train, v = 2)
rf_grid <- grid_regular(
mtry(range = c(2, 15)),
min_n(),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./RandomForestWedn.csv", delim = ",")
rf_spec <- rand_forest(
mtry = tune(),
trees = 500,
min_n = tune()
) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_workflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(my_recipe2)
cv_splits <- vfold_cv(train, v = 3)
rf_grid <- grid_regular(
mtry(range = c(2, 15)),
min_n(),
levels = 5
)
rf_results <- tune_grid(
rf_workflow,
resamples = cv_splits,
grid = rf_grid,
metrics = metric_set(accuracy)
)
best_rf <- rf_results %>%
select_best(metric = "accuracy")
final_rf_workflow <- rf_workflow %>%
finalize_workflow(best_rf)
final_rf_model <- final_rf_workflow %>%
fit(data = train)
test_predictions <- predict(final_rf_model, new_data = test, type = "class")
submission <- test %>%
select(Id) %>%
bind_cols(Cover_Type = test_predictions$.pred_class)
vroom_write(submission, "./RandomForestWedn.csv", delim = ",")
